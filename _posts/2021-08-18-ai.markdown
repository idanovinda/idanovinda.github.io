---
layout: default
modal-id: 3
# date: 2014-07-18
img: ai.png
alt: image-alt
# project-date: April 2014
title: AI Projects
# client: Start Bootstrap
# category: Web Development
description:  </br></br><ol><h3><li>Understanding Knowledge distillation of pre-trained language models</h3></li><p></br>The use of pre-trained language models has been a great success in natural language processing tasks. However, these models usually have a large number of parameters that are computationally expensive to fine-tune. Moreover, they are not practical to be deployed in low-end and edge devices such as mobile phones due to high latency. Many knowledge distillation techniques have been proposed to compress the model size while retaining the large modelâ€™s performance. Despite the popularity of knowledge distillation in pre-trained language models, the properties that define the success of its distillation have not been explored. In this work, we investigate the role of distillation variables in distilling pre-trained language models. We evaluate the effect of the variable choice in factual knowledge and linguistic tasks. Understanding the role of distillation variables could open an opportunity for optimizing the distillation process in pre-trained language models.</br></br></p> <h3><li>News Classification</li></h3><p></br>In this project, we proposed new method for news classification in <a href="https://pytorch.org/text/stable/datasets.html#ag-news">AGNews data</a>. We combined convolutional neural network and gated recurrent units to extract the text features then classify it. Our new method can outperform the baseline method with 2% of accuracy and 92% of final accuracy.</br></br></p> <h3><li>Robustness of Watermark in Neural Network Model</li></h3><p></br>When it takes a lot of effort and resources to train a neural network with high accuracy, it is reasonable to seek for a reliable mechanism to protect the model from being stolen. Only recently, different watermarking techniques have been proposed that enable remote ownership verification. One approach trains the model on watermarked data such that the owner can reveal the  predefined outcome when querying a stolen black-box model with watermarks. This project examines the question of robustness of the embedded watermarks when the attacker knows the type of the watermark used. The experiments show that the watermarks can be removed with little effort, if the attacker has some knowledge on the parameters of the used watermark technique. The decrease in test accuracy on the dataset mostly depends on the attacker having access to original training data.</br></br></p> <h3><li>Description's Answer Classification</li></h3><p></br>While considered to be more effective method to measure student's performance, evaluating description question's answer from many students is a tiresome task. Hence, such automatic system that is based on text classification need to be developed in order to return the evaluation result to students in shorter time.  This research focused on utilization of GloVe word vector representation and Convolutional Neural Network to classify student's answer on description question based on given solution. Study was done on a set of students' answer from Indonesian Ministry of Culture and Education, 2408 answer in total. Variation on several hyper parameter is done to conclude the model's behaviour on this kind of problem and dataset. Our final model scores 89,36\% accuracy on 5 fold cross-validation.</br></br></p></ol>
---
